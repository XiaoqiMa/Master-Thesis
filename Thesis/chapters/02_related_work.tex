In this chapter, previous works in areas that are related to the topic of this thesis will be covered. Firstly, we will introduce the model interpretation methods, which facilitate us to interpret predictions from black box models. Two groups of methods, including global interpretation methods and local interpretation methods, are explained respectively. And the last part will give an overview of subgroup discovery technique, focusing on the target concept and interestingness measure. 
%The next part will investigate the decision trees, which could provide us a human-understandable explanation to the prediction. 
\section{Related works}

%Global vs. Local. Global interpretability implies knowing what patterns are present in general
%(such as key features governing galaxy formation), while local interpretability implies knowing the reasons for a specific decision (such as why a particular loan application was rejected). The former may be important for when scientific understanding or bias detection is the goal; the latter when one needs a justification for a specific decision.

\subsection{Model interpretation methods}
%there are vast numbers of papers that have imbued interpretability in their methodology
Recently, studies trying to understand why a model makes a certain prediction have been intensively investigated. The extent to which the model or its prediction is human-understandable is termed as Interpretability (or comprehensibility), whose definition can be found in paper \cite{kim2016examples}. Various criteria can be used to classify types for machine learning interpretability. Intrinsic interpretability, for example, is one type of the interpretability, which refers to models that are intrinsic interpretable owing to their simple structures, such as linear models or decision trees. In contrast, post hoc interpretability is meant to analyze the model interpretability after model training. Particularly, post hoc interpretability is mainly considered. And to address the measurement of model interpretability, the complexity of the predictive model in terms of the model size is a determining factor as referenced from \cite{freitas2014comprehensible}. As noticed, it is much easier to explain model predictions when the predictive model has high comprehensibility, which is called the interpretable model. Usually, the interpretable model could help us directly explain the predictions with its parameters, and the commonly used models are linear regression, logistic regression and the decision trees. However, black box models cannot be interpreted through their inner parameters due to the low interpretability. Therefore, considerable research efforts have been devoted to model interpretation methods which could provide decent explanations that are understandable to experts. Generally, two broad genres of explanation methods are often mentioned, one is the global interpretation methods and the other refers to local interpretation methods.


\subsubsection{Global interpretation}

%SHAP feature importance is an alternative to permutation feature importance. There is a big difference between both importance measures: Permutation feature importance is based on the decrease in model performance. SHAP is based on magnitude of feature attributions.

The global interpretation method focus on the global view of the input variables, more specifically, it points out the most significant features that can affect predictions of the entire data set. As mentioned in \cite{friedman2001greedy}, the Partial Dependence Plot (PDP) is a global interpretation method which shows the marginal effect of a feature on the predictions of the machine learning model. It can show the relationship between the selected feature and the target by adapting the values of the selected feature, and specifically to characterize the feature influence on model predictions. 

Another popular approach is called feature importance. There are many methods for assessment of feature importance. The default feature importance mechanism was proposed and implemented by the inventor of the RandomForest algorithm, which was to add up the Gini decreases for each variable over all trees in the forest and get the average. However, pointed by Strobl et al \cite{strobl2007bias}, this method was biased and was not reliable in scenarios when the selected variable was biased in terms of the scale of measurement. Later, a novel strategy called permutation importance was described \cite{fisher2018model}. In this approach, the feature importance was estimated by the drop of prediction accuracy of the model after permuting the selected feature. A feature is regarded as ”important” if prediction accuracy drops a lot after shuffling feature values as the model depends on the feature for the prediction. Conversely, a feature is ”unimportant” if the accuracy is slightly dropped, which means the feature is hardly relied on for the model.

%Influences do not provide any explanations about how the variable actually affects the response
\subsubsection{Local Interpretation}

Local interpretation methods aim at the instance level explanation which means each instance should be supplied with a corresponding explanation identifying the cause to prediction. Following this idea, it leads us to the local surrogate methods, which are able to explain individual predictions of any black box models faithfully. As a concrete implementation of local surrogate models, Local interpretable model-agnostic explanations (LIME) was initially proposed in paper \cite{ribeiro2016should}. Another possible approach is to calculate the individual contribution of each feature in an instance to compose the final prediction as described in paper \cite{robnik2008explaining}. Inspired by this idea and the theoretical knowledge from the coalitional game theory, Shapely value was proposed to explain instance-level predictions with contributions of each feature values \cite{kononenko2010efficient}. However, in this approach, the explanation for the prediction of a black box model is just a simple value, rather than an explanation model like LIME, which fails to make judgments about the connections between input change and prediction change. To address those problems, Lundberg and Lee \cite{lundberg2017unified} proposed a unified framework for explaining predictions, which is based on the Shapley value, and they named it SHAP(SHapley Additive exPlanations). This novel approach unified existing explanation methods and brought more clarity to the method space. They introduced the explanation model by treating the explanation of an individual prediction as a model.



\subsection{Subgroup discovery overview}

Recent developments of the research filed in knowledge discovery in databases have attracted much attention, where a numerous of methods are proposed to extract local patterns from large volumes of data  \cite{fayyad1996data}. Apart from the methods for mining local patterns such as discriminative patterns \cite{cheng2008direct} and emerging patterns \cite{dong1999efficient}, subgroup discovery (also called pattern mining) is established as a supervised and descriptive data mining technique. As defined in \cite{herrera2011overview}, in the subgroup discovery task, assuming we have a population of individuals and the corresponding property of interest, it aims to discover subgroups that are statistically "most interesting". Specifically, the interesting subgroups have the most unusual distributional characteristics with respect to certain property of interest given by the target variable \cite{atzmueller2009fast}.

In a formal definition, the fundamental concepts of subgroup discovery task could be summarized by a quadruple (D, $\Sigma$, T, Q) \cite{lemmerich2014novel}. In the quadruple, D represents the dataset, which is formed by a group of instances. $\Sigma$ means the search space, consisting of a set of selection expressions, and the search space covers all the patterns that are traversed through. T implies the target concepts being exploited in pattern mining task. Commonly, a single target concept, e.g. binary or numeric, is applied in the task, nevertheless, multi-target concepts are also enabled given by the exceptional model mining framework \cite{leman2008exceptional}. Concerning the quality measure criteria, symbolled as Q, it is specified depending on the target concept. 

Considerable research efforts have been devoted to study the binary target concept. Normally the quality measure for binary target relies on the parameters contained in a contingency table, which describes the distribution of positive/negative instances for the observed pattern and its complement, respectively. In paper \cite{klosgen1996explora}, Kloesgen proposed a prevalent family of quality measure, relating to the size of the subgroup and the difference between the share of the target concept in the subgroup and the general population. Correspondingly, several approaches to measure the quality of numeric attributes have been proposed, and a listing of interestingness measures for numeric target concepts could be found in \cite{pieters2010subgroup}. Since numeric attribute has certain characteristics, such as mean value or median value, therefore, the quality measure for numeric target could be formalized by slightly adapting the quality function for binary targets. In specific, the share of target in the subgroup and in the entire population could be replaced by the characteristic of the target. Generally, there are five categories of interestingness measure for numeric target, concluded in \cite{lemmerich2014novel}, which are mean-based measures, median-based measures, variance-based measures, distribution-based measures, and rank-based measures. Furthermore, as for multi-target concept, the quality function has been described in a number of studies. And a general framework for multi-target quality functions is given by exceptional model mining \cite{leman2008exceptional}, proposing a variety of model classes, which contains the correlation model, the regression model and the classification model.  

In the subgroup discovery task, unlike the choice of applied quality measure which is mainly determined by the target concept, the mining algorithms are almost equivalent. And for a specific algorithm, the three algorithmic components should be defined, which are enumeration strategy, data structure and pruning strategy. Various enumeration strategies could be used, e.g. exhaustive methods, seeking to acquire the optimal subgroup by traversing through the whole search space. In contrast, heuristic approaches, normally a beam search strategy \cite{clark1989cn2}, is often used for subgroup discovery due to its efficiency, which aims to find interesting patterns but not necessarily the optimal patterns in a short time. Normally, data is stored in a horizontal layout, e.g. tabular-formatted database. Instead, vertical data representations can also be used, which is covered in paper \cite{zaki2000scalable}. Besides, FP-tree structure is also applicable referring to the wide-spread FP-Growth algorithms \cite{han2000mining}. Furthermore, considering the efficiency of algorithms, the pruning strategies if of critical importance. To determine the upper-quality bounds and safely prune parts of the search space, optimistic estimates could be applied as proposed in \cite{wrobel1997algorithm} for binary target concept. In addition, to shrink the search space of subgroup discovery task, minimal support pruning strategy is useful by exploiting anti-monotone constraints.  



