In this chapter, previous works in areas that are related to the topic of this thesis will be covered. Firstly, we will introduce the model interpretation methods, which facilitate us to interpret predictions from black box models. Two groups of methods, including global interpretation methods and local interpretation methods are explained respectively. The next part will investigate the decision trees, which could provide us a human-understandable explanation to the prediction. And the last part will give an overview of subgroup discovery technique, focusing on the target concept and interestingness measure. 

\section{Related works}


\subsection{Model interpretation methods}

\subsubsection{Global interpretation}

Default feature importance mechanism
%	(https://explained.ai/rf-importance/index.html#3)
The most common mechanism to compute feature importances, and the one used in scikit-learn's RandomForestClassifier and RandomForestRegressor, is the mean decrease in impurity (or gini importance) mechanism (check out the Stack Overflow conversation). The mean decrease in impurity importance of a feature is computed by measuring how effective the feature is at reducing uncertainty (classifiers) or variance (regressors) when creating decision trees within RFs. The problem is that this mechanism, while fast, does not always give an accurate picture of importance. Breiman and Cutler, the inventors of RFs, indicate that this method of “adding up the gini decreases for each individual variable over all trees in the forest gives a fast variable importance that is often very consistent with the permutation importance measure.” (Emphasis ours and we'll get to permutation importance shortly.)

Permutation importance
\begin{itemize}
	\item Feature importance
	\item Partial dependence plot
\end{itemize}

\subsubsection{Local Interpretation}

\subsection{Decision Trees}


\subsection{Subgroup discovery overview}

Recent developments of the research filed in knowledge discovery in databases have attracted much attention, where a diverse of methods are proposed to extract local patterns from large volumes of data  \cite{fayyad1996data}. Apart from the methods for mining local patterns such as discriminative patterns \cite{cheng2008direct} and emerging patterns \cite{dong1999efficient}, subgroup discovery (also called pattern mining) is established as a supervised and descriptive data mining technique. As defined in \cite{herrera2011overview}, in the subgroup discovery task, assuming we have a population of individuals and the corresponding property of interest, it aims to discover subgroups that are statistically "most interesting". Specifically, the interesting subgroups have the most unusual distributional characteristics with respect to certain property of interest given by the target variable \cite{atzmueller2009fast}. 

In a formal definition, the fundamental concepts of subgroup discovery task could be summarized by a quadruple (D, $\Sigma$, T, Q) \cite{lemmerich2014novel}. In the quadruple, D represents the dataset, which is formed by a group of instances. $\Sigma$ means the search space, consisting of a set of selection expressions, and the search space covers all the patterns that are traversed through. T implies the target concepts being exploited in pattern mining task. Commonly, a single target concept, e.g. binary or numeric, is applied in the task, nevertheless, multi-target concepts are also enabled given by the exceptional model mining framework \cite{leman2008exceptional}. With respect to the quality measure criteria, symbolled as Q, it is specified depending on the target concept. 

Considerable research efforts have been devoted to binary target concept. Normally the quality measure for binary target is relied on the parameters contained in a contingency table, which describes the distribution of positive/negative instances for the observed pattern and its complement, respectively. In paper \cite{klosgen1996explora}, Kloesgen proposed a prevalent family of quality measure, relating to the size of the subgroup and the difference between the share of the target concept in the subgroup and the general population. Correspondingly, several approaches to measure the quality of numeric attributes have been proposed, and a listing of interestingness measures for numeric target concepts could be found in \cite{pieters2010subgroup}. Since numeric attribute has certain characteristics, such as mean value or median value, therefore, the quality measure for numeric target could be formalized by slightly adapting the quality function for binary targets. In specific, the share of target in the subgroup and in the entire population could be replaced by the characteristic of the target. Generally, there are five categories of interestingness measure for numeric target, concluded in \cite{lemmerich2014novel}, which are mean-based measures, median-based measures, variance-based measures, distribution-based measures, and rank-based measures. Furthermore, as for multi-target concept, the quality function has been described in a number of studies. And a general framework for multi-target quality functions is given by exceptional model mining \cite{leman2008exceptional}, proposing a variety of model classes, which contains the correlation model, the regression model and the classification model. 

In the subgroup discovery task, unlike the choice of applied quality measure which is mainly determined by the target concept, the mining algorithms are almost equivalent. And for a specific algorithm, the three algorithmic components should be defined, which are enumeration strategy, data structure and pruning strategy. Various enumeration strategies could be used, e.g. exhaustive methods, seeking to acquire the optimal subgroup by traversing through the whole search space. In contrast, heuristic approaches, normally a beam search strategy \cite{clark1989cn2}, is often used for subgroup discovery due to its efficiency, which aims to find interesting patterns but not necessarily the optimal patterns in a short time. Normally, data is stored in horizontal layout, e.g. tabular-formatted database. Instead, vertical data representations can also be used, which is covered in paper \cite{zaki2000scalable}. In addition, FP-tree structure is also applicable referring to the wide-spread FP-Growth algorithms \cite{han2000mining}. Furthermore, considering the efficiency of algorithms, the pruning strategies if of critical importance. To determine the upper quality bounds and safely prune parts of the search space, optimistic estimates could be applied as proposed in \cite{wrobel1997algorithm} for binary target concept. In addition, to shrink the search space of subgroup discovery task, minimal support pruning strategy is useful by exploiting anti-monotone constraints. 



