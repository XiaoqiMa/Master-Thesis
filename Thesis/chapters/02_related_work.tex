In this chapter, related work... \cite{eli5}

\section{Related work and Applications}


\subsection{Model interpretation methods}

\subsubsection{Global interpretation}

Default feature importance mechanism
%	(https://explained.ai/rf-importance/index.html#3)
The most common mechanism to compute feature importances, and the one used in scikit-learn's RandomForestClassifier and RandomForestRegressor, is the mean decrease in impurity (or gini importance) mechanism (check out the Stack Overflow conversation). The mean decrease in impurity importance of a feature is computed by measuring how effective the feature is at reducing uncertainty (classifiers) or variance (regressors) when creating decision trees within RFs. The problem is that this mechanism, while fast, does not always give an accurate picture of importance. Breiman and Cutler, the inventors of RFs, indicate that this method of “adding up the gini decreases for each individual variable over all trees in the forest gives a fast variable importance that is often very consistent with the permutation importance measure.” (Emphasis ours and we'll get to permutation importance shortly.)

Permutation importance
\begin{itemize}
	\item Feature importance
	\item Partial dependence plot
\end{itemize}

\subsubsection{Local Interpretation}

\subsection{Subgroup discovery overview}


\subsection{Applications}






