After a detailed introduction to the methodology, in this chapter, we will concentrate on practical experiments. To begin with, datasets that will be utilized in the experiments will be introduced, including the well-known UCI datasets and the artificial dataset. Next, a general overview of the classification or regression models that will be encountered in the experiments is provided. Then, we intend to compare different local interpretation methods on the same dataset and give explanations on the feature that is interested. To interpret the feature explanation for an individual instance and discover the interesting patterns, the subgroup discovery technique is applied. As a comparison, decision tree visualization is exposed to find local patterns. And finally, we will apply the promising SHAP approach to conduct case studies on real datasets. 

\subsection{Datasets}


\subsubsection{Artificial dataset}

Before exploring the local interpretation methods on real datasets, we would like to justify the concept that interesting subgroups could be recovered from the artificial dataset by inspecting variable influence. Presumably, there were hidden patterns in the synthetic dataset that were useful to provide a reasonable explanation for the predictions. By interpreting the effect of a certain variable, e.g. gender, it was assumed that the interesting pattern could be recovered. The procedure to construct an artificial dataset and conduct experiments will be described as follows. 

For simplicity, we constructed the artificial dataset relying on the popular "Adult Income" dataset, but we only extracted partial information, which meant that only the information about age, education-num, sex, hours-per-week and income were included. As assumed, the synthetic data contained some interesting patterns, such as "age < 30". One exemplary case was that when "age < 30", the attribute "gender" had a stronger effect on predictions while in its complementary subgroup, the effect of "gender" was slight. And the task was indeed to discover this pattern by exploring the effect of gender. 

For further experiments, one way to fabricate this interesting pattern was to modify the gender effect directly on the corresponding subgroups. For instance, if the condition that "age < 30" was met, we could manually add 3 unit in terms of the scale of measurement on gender effect, and otherwise we could subtract 3 unit. Another idea was to establish two models that behaved differently when considering this condition. It is known that the coefficients in the logistic regression model have straightforward interpretation, indicating the influence level by the input features. Therefore, we could create two distinct models by changing the weights of the features in accordance with the previously defined patterns, which was that when "age < 30", the effect of gender was relatively large. In specific, we could assign larger weights to the model that was applied to the pre-described subgroup to maintain larger gender effect, while decreasing feature weights on the model that was applied to its complementary subgroup.

In this paper, we would like to adopt the latter method to make up the synthetic dataset and build the models. 

\subsubsection{UCI datasets}

Apart from the synthetic dataset, we will mostly consider datasets that could be found in UCI Machine Learning Repository \cite{asuncion2007uci}\cite{Dua:2019}. Ideally, we would like to choose datasets that cover various domains, including social, financial and life science areas. Therefore, for classification tasks, concerning the popularity and quality of datasets, we decided to adopt the "Adult Income", "German Credit", and "Breast Cancer Wisconsin" datasets. In Adult Income dataset, there are 14 descriptive features and more than 40 thousand instances, which were extracted from the US Census database. And the task was to predict whether a person earned more than 50K a year or not. As for the German Credit dataset, it was determined to figure out whether a person had good or bad credit risks relying on the 20 descriptive attributes for each person. It is worth mentioning that these two datasets contain multivariate data types, consisting of categorical features and numerical features. In that regard, data preprocessing needs to be considered in addition. Another Breast Cancer dataset is composed of 32 features and all of them are numerical features except for the predicted label which tells whether the diagnosis of cancer is malignant or benign. Those features of an individual instance are extracted from an image of a breast mass, which describes the characteristics of the cell nuclei in the image. 

For regression tasks, we specifically choose the "Bike Sharing" and "Boston Housing" datasets. In Bike sharing dataset, the task is to predict the count of total rental bikes within a specific time frame. It is made up of 17389 entries and each with 16 distinct features. Regarding the Boston housing dataset, it is derived from US census service concerning housing price in the area of Boston MA. 505 records can be found in the dataset and each record contains 14 numerical features. 

In summary, a general overview of real-world datasets that will be used in experiments is concluded in Table ~\ref{tab:datasets}

\begin{table}[H] 
	\centering 
	\caption{Datasets used in experiments}
	\label{tab:datasets}
	\ra{1.3}
	\begin{tabular}{{m}{3cm}cccc}\toprule[0.5mm]
		Datasets & Usage & \#Instances & \#Features \\ 
		\midrule[0.3mm]
		Adult Income & Classification & 48842 & 14 \\
		German Credit & Classification & 1000 & 20 \\
		Breast Cancer & Classification & 569 & 32 \\
		Bike Sharing & Regression & 17389 & 16 \\
		Boston Housing & regression & 505 & 14 \\
		\bottomrule[0.5mm]
	\end{tabular}
	
\end{table}


%Samples contain 13 attributes of houses at different locations around the Boston suburbs in the late 1970s. Targets are the median values of the houses at a location (in k$).

%\subsection{Datasets description}
%
%\subsubsection{Artificial dataset}
%
%\subsubsection{UCI datasets}



%\subsection{Data preprocessing }
%To proceed with the analysis, data has to be properly processed. At first


\subsection{Experiments setup}


\subsubsection{Machine learning models}

The first machine learning algorithm that will be used in experiments is Random Forests, which are an ensemble method for classification or regression tasks by creating multiple decision trees at the training time. In this algorithm, it uses bagging and feature randomness when constructing each individual tree. And the final prediction is decided by the voting among a large number of independent trees \cite{breiman2001random}. The key concept behind this algorithm is that a large number of relatively uncorrelated trees operating as a committee will outperform any of the individual constituent models. Typically, random forests are treated as a black box model since it is nigh infeasible to gain a full understanding of the decision process by examining each tree. Commonly, the implementation of this algorithm in the scikit-learn library is adopted. 

Another black box model is Gradient Boosting Trees, which also construct an ensemble of decision trees to perform classification or regression tasks, where each decision tree is a weak prediction model. However, unlike Random Forests algorithm that fully grown decision trees are created, in Gradient Boosting Trees algorithm, each tree is a shallow tree, sometimes even as small as decision stumps (trees with two leaves). The main idea behind is to add new decision trees to the ensemble sequentially. At each iteration of the training process, those data instances with high prediction errors are emphasized by the next decision tree in order to correct the errors. And the final prediction is determined by the weighted average for each decision tree, where the weight depends on a performance of the corresponding tree \cite{natekin2013gradient}. 

There are a rich variety of libraries that implement the gradient boosting trees algorithms. In this thesis, two efficient and scalable implementations are mainly adopted, one is called XGBoost \cite{chen2016xgboost} and the other is LightGBM \cite{ke2017lightgbm}. XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable and it was a part of winning solutions of multiple machine learning competitions \cite{xgboost}. The library also works natively with scipy sparse data format and can convert it to an internal data format, called DMatrix, which speeds up the training process. Comparatively, LIghtGBM also implements fast, distributed, high performance gradient boosting algorithms. As claimed, it can outperform existing frameworks on both efficiency and accuracy with significantly lower memory consumption \cite{lightgbm}. 

Recently, Rudin pointed out that people have a blind belief in the myth of the accuracy-interpretability trade-off, meaning that there is a widespread acceptance that more complex models have higher performance \cite{rudin2019stop}. And that is one of the reasons why neural network is applied to many fields and is believed to provide the state-of-the-art performance. Since we aim to interpret any kind of black box models in this framework, thereby we believe it is worth to investigate neural networks. There are many types of architectures for neural networks, but for simplicity, full-connected layers neural network is chosen for the experiment. 

On the other hand, we design the framework to be compatible with text classification. In text classification task, we usually process the text as sequence of words. However, it is noticed that the fully-connected layers neural network fails to process sequential data efficiently. Therefore, we determine to use an architecture called Long short-term memory network (LSTM), which is an variation based on recurrent neural networks.


%a function that is too complicated for any human to comprehend or (2) a function that is proprietary (Supplementary Section A). Deep learning models, for instance, tend to be black boxes of the first kind because they are highly recursive.

%Here we would just apply the GBMs to the available dataset without any manipulations on its features, or any external expert-driven knowledge involved.

\subsection{Recover patterns on artificial dataset} 

%Before exploring the local interpretation methods, we would like to justify the concept that interesting subgroups could be recovered from the artificial dataset by inspecting variable influence. Presumably, there were hidden patterns in the synthetic dataset that were useful to provide reasonable explanation to the predictions. By interpreting the effect of a certain variable, e.g. gender, it was assumed that the interesting pattern could be recovered. The procedure to construct an artificial dataset and conduct experiments will be described as follows. 
%
%For simplicity, we constructed the artificial dataset relying on the popular adult dataset, but we only extracted partial information, which meant that only the information about age, education-num, sex, hours-per-week and income were included. As assumed, the synthetic data contained some interesting patterns, such as "age < 30". One exemplary case was that when "age < 30", the attribute "gender" had stronger effect on predictions while in its complementary subgroup, the effect of "gender" was slight. And the task was indeed to discover this pattern by exploring the effect of gender. For further experiments, one way to fabricate the interesting pattern was to modify the gender effect directly on the corresponding subgroups. For instance, if the condition that "age < 30" was met, we could manually add 3 unit in terms of the scale of measurement on gender effect, and otherwise we could subtract 3 unit. Another idea was to establish two models that behaved differently when considering this condition. It is known that the coefficients in the logistic regression model have straightforward interpretation, indicating the influence level by the input features. Therefore, we could create two distinct models by changing the weights of the features in accordance with the previous defined patterns. In specific, we could assign larger weights to the model that was applied  to the pre-described subgroup to maintain larger gender effect, while decreasing feature weights on the model that was applied to its complementary subgroup.

As clarified earlier, the artificial dataset was constructed based on the Adult dataset with a hidden pattern indicating that the gender had a large impact on the prediction when "age < 30". Therefore, the aim was trying to verify whether this interesting subgroup could be recovered by pattern mining technique. 

Firstly, to measure the gender effect, we could simply use the binary flip approach described in the previous chapter. By flipping the gender value, i.e. transform from "male" to "female" or the other way around, the prediction change denoted as probability was calculated and roughly it was regarded as the effect of gender. Then, treating the effect of gender as the target concept, the subgroup discovery technique was applied to the artificial dataset to discover interesting subgroups. It could be observed that these interesting subgroups include the subgroup that was artificially generated in the dataset. The detailed results were left to the next chapter. In conclusion, it could be proved that the subgroup discovery technique could indeed provide us patterns of explanations that facilitate us to understand the predictions. 


\subsubsection{Comparison of different local interpretation methods}

In previous chapter, we have already introduced several local interpretation methods, and in this subsection, we would like to have a detailed comparison of those methods. In this experiment, breast cancer dataset is selected.  First of all, concerning the influence by the interactions between features, related features should be dropped through feature processing. Generally, the pearson correlation coefficient between features are explored and some highly correlated features are excluded. Since we intend to inspect the variable influence, it is better to have an overview about the feature importance. Thereby, it is aimed to explore the impact of the most important feature by various local interpretation methods. 

In this case, since we know that the dataset contains only the numeric features, we would first use the numeric perturbation method to estimate the impact of the most important feature for a specific instance. Then LIME could be utilized to fit a ridge regression model for the selected instance, and the coefficients represent the feature weight, which provides implications for the explanation. In contrast, Kernel SHAP calculates the shapley value for each feature value in this instance by considering all feature combinations and those shapley values contributes to the final explanation. 


\subsubsection{Comparison between decision tree and subgroup discovery}

For the purpose of recognition of patterns in dataset, data mining techniques are considered. In this part, we would like to observe the similarity and differences between two data mining techniques, which are decision tree algorithm and subgroup discovery technique. In principle, decision tree algorithm is considered as an interpretable model, whereas it could also be used to mine local patterns through the decision path, where each path is traversed from the root node to a leaf node. Since we desire to observe the pattern in data where the inspected variable has significant influence, we should use the impact of that feature as the label for each instance. And this label is a numeric value which could be measured by the local interpretation methods. As for the subgroup discovery technique, it is aimed to discover interesting patterns from the data with respect to the target. In this case, the target is the impact value of the feature. 


\subsubsection{Case Study}

As claimed at the beginning, this interpretation framework supports explaining model-agnostic black box models on tabular data and textual data. In this part, we would like to show two case studies and conduct experiments on tabular data and textual data respectively. 

\textbf{Case study 1: Adult Income dataset}

As described, in Adult Income dataset, there are 14 descriptive features and more than 40 thousand instances. And the label indicates whether a person earns more than 50K dollars per year or not. Of course, the first step is to process the dataset when it is already available. In this case, for the convenience of training black box model, label encoding technique was applied to those categorical features. To have a better understanding of those features, the feature importance was measured. Even though there are many approaches to measure the feature importance, in this thesis, two methods were mainly used. One way was to calculate the permutation feature importance score and rank by the score. Another idea was to computer the sum of the absolute shapley values for each feature, and the ranking order could be observed from the corresponding summary plot. 

Next, we would like to inspect the influence of variable "Sex". We use three approaches to measure the impact of attribute "Sex". Firstly, we calculate the prediction change of attribute "sex" by binary flip approach, in this way, we could get a value indicating the influence for each instance. Secondly, by fitting a linear model through LIME, we could identify the weight for feature "Sex", which implied degree of impact. Lastly, the contribution of attribute "Sex" was estimated by shapley value by computing the marginal effect while considering all combinations of feature values. 

Subsequently, after obtaining the impact score for feature "Sex" by various methods, subgroup discovery technique was applied to the dataset under the condition that taking the contribution value of feature "Sex" as the target. Afterwards, several interesting patterns could be discovered. 


\textbf{Case study 2: Amazon review dataset}
%Text classification or document classification is one of the most important tasks in Natural Language Processing field. Its primary goal is predicting the label or a class of a given document based on words, characters or other available features. Text classification has been extensively studied, and it has been used in a various application, the most famous one would be detecting spam letters in email clients [59]. Analyzing the content of the news articles or blog posts is also often used to make predictions about the prices on stock markets and currency exchange rates.
%The problem of text classification consists of two stages: text preprocessing and classification. The preprocessing stage is converting given text to a fixed represen- tation, and the classification stage learns the class of the text based on this represen- tation. Both steps are necessary and require a handful of design choices to produce adequate results.
