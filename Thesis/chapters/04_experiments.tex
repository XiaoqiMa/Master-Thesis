Experiments


\section{Datasets}


\subsection{Datasets description}

\subsection{Data preprocessing (experiment settings)}


\section{Experiments}
%Another machine learning algorithm that will be used with produced feature vectors is gradient boosting trees [110]. The algorithm creates an ensemble of decision trees that perform classification or regression, where each tree is a weak prediction model. A decision tree is a tree-like graph, where each node represents the split based on one of the features in a feature vector. Figure 12 shows an example with two trees, the nodes represent the ‘decisions’ that split the data into subgroups. In case of ensemble models, the trees are called weak because they are intentionally limited by depth [110].
%The gradient boosting trees algorithm requires definition of loss function that can be minimized. The algorithm is called boosting because it builds decision trees iteratively. It creates the first tree and finds the data points with high prediction errors, this errors show which data points should be emphasized by the next decision tree. After building all the decision trees, their predictions of these trees are combined by weighted average, where weight depends on a performance of a tree. Different approaches of building gradient boosting trees depend on many hyperparameters and is an active area of research [112]. More detailed description of this approach is beyond the scope of this thesis.
%The library that is used for gradient boosting trees in this thesis is called XGBoost [111]. The gradient boosting trees algorithm implemented in XGBoost were a part of winning solutions of multiple machine learning competitions [113], including competitions in natural language processing field. The library also works natively with scipy sparse data format and can convert it to an internal data format, called DMatrix, which speeds up the training process. The main parameters of the algo- rithm are max_depth, num_boost_round and learning_rate. The maximal number of splits that allowed in a tree is defined by max_depth, and the number of trees in an ensemble is determined by num_boost_round. The learning_rate parameter enables regularization of the model, by forcing the algorithm to build more trees to achieve the same score. The XGBoost library also allows the usage of validation set, to stop training if the score on validation data did not improve for a given amount of boosting rounds.
%The gradient boosting trees algorithm is a non-linear algorithm, which means that it can approximate more complex data, but without proper regularization can easily overfit. The non-linear nature of the algorithm means signifies that there is no linear dependency between features (i.e., words) and the target class. But because during the construction of decision trees only the most significant features are used, it is possible to sort features by their importance to a given task. Therefore, in our task, we will be able to use this functionality to extract words that had the most influence in predicting the gender based on the text of the comment.

\subsection{Artificial Datasets}
artificial dataset to recover subgroups 

\subsection{Comparison of different local interpretation methods}

datasets: adult, credit-g, housing
binary flip: perturbation, LIME, SHAP

numeric perturbation: perturbation, LIME, SHAP

classification vs. regression

decision tree vs. subgroup discovery

\subsection{Case Study}

real world case study 2-3 datasets