After a detailed introduction to the methodology, in this chapter, we will concentrate on practical experiments. To begin with, datasets that will be utilized in the experiments will be introduced, including the well-known UCI datasets and the artificial dataset. Next, a general overview of the classification or regression models that might be encountered in the experiments is provided. Then, we intend to compare different local interpretation methods on the same dataset and give explanations on the feature that is interested. To interpret the feature explanation for an individual instance and discover the interesting patterns, the subgroup discovery technique is applied. As a comparison, decision tree visualization is exposed to find local patterns. And finally, we will apply the promising SHAP approach to conduct case studies on real datasets. 

\section{Datasets}


\subsection{Artificial dataset}

Before exploring the local interpretation methods on real datasets, we would like to justify the concept that interesting subgroups could be recovered from the artificial dataset by inspecting variable influence. Presumably, there were hidden patterns in the synthetic dataset that were useful to provide a reasonable explanation for the predictions. By interpreting the effect of a certain variable, e.g. gender, it was assumed that the interesting pattern could be recovered. The procedure to construct an artificial dataset and conduct experiments will be described as follows. 

For simplicity, we constructed the artificial dataset relying on the popular "Adult Income" dataset, but we only extracted partial information, which meant that only the information about age, education-num, sex, hours-per-week and income were included. As assumed, the synthetic data contained some interesting patterns, such as "age < 30". One exemplary case was that when "age < 30", the attribute "gender" had a stronger effect on predictions while in its complementary subgroup, the effect of "gender" was slight. And the task was indeed to discover this pattern by exploring the effect of gender. 

For further experiments, one way to fabricate this interesting pattern was to modify the gender effect directly on the corresponding subgroups. For instance, if the condition that "age < 30" was met, we could manually add 3 unit in terms of the scale of measurement on gender effect, and otherwise we could subtract 3 unit. Another idea was to establish two models that behaved differently when considering this condition. It is known that the coefficients in the logistic regression model have straightforward interpretation, indicating the influence level by the input features. Therefore, we could create two distinct models by changing the weights of the features in accordance with the previously defined patterns, which was that when "age < 30", the effect of gender was relatively large. In specific, we could assign larger weights to the model that was applied to the pre-described subgroup to maintain larger gender effect, while decreasing feature weights on the model that was applied to its complementary subgroup.

In this paper, we would like to adopt the latter method to make up the synthetic dataset and build the models. 

\subsection{UCI datasets}

Apart from the synthetic dataset, we will mostly consider datasets that could be found in UCI Machine Learning Repository \cite{asuncion2007uci}\cite{Dua:2019}. Ideally, we would like to choose datasets that cover various domains, including social, financial and life science areas. Therefore, for classification tasks, concerning the popularity and quality of datasets, we decided to adopt the "Adult Income", "German Credit", and "Breast Cancer Wisconsin" datasets. In Adult Income dataset, there are 14 descriptive features and more than 40 thousand instances, which were extracted from the US Census database. And the task was to predict whether a person earned more than 50K a year or not. As for the German Credit dataset, it was determined to figure out whether a person had good or bad credit risks relying on the 20 descriptive attributes for each person. It is worth mentioning that these two datasets contain multivariate data types, consisting of categorical features and numerical features. In that regard, data preprocessing needs to be considered in addition. Another Breast Cancer dataset is composed of 32 features and all of them are numerical features except for the predicted label which tells whether the diagnosis of cancer is malignant or benign. Those features of an individual instance are extracted from an image of a breast mass, which describes the characteristics of the cell nuclei in the image. 

For regression tasks, we specifically choose the "Bike Sharing" and "Boston Housing" datasets. In Bike sharing dataset, the task is to predict the count of total rental bikes within a specific time frame. It is made up of 17389 entries and each with 16 distinct features. Regarding the Boston housing dataset, it is derived from US census service concerning housing price in the area of Boston MA. 505 records can be found in the dataset and each record contains 14 numerical features. 

In summary, a general overview of real-world datasets that will be used in experiments is concluded in Table \ref{datasets}

\begin{table}[H] \label{datasets}
	\centering 
	\caption{Datasets used in experiments}
	\ra{1.3}
	\begin{tabular}{{m}{3cm}cccc}\toprule[0.5mm]
		Datasets & Usage & \#Instances & \#Features \\ 
		\midrule[0.3mm]
		Adult Income & Classification & 48842 & 14 \\
		German Credit & Classification & 1000 & 20 \\
		Breast Cancer & Classification & 569 & 32 \\
		Bike Sharing & Regression & 17389 & 16 \\
		Boston Housing & regression & 505 & 14 \\
		\bottomrule[0.5mm]
	\end{tabular}
	
\end{table}


%\subsection{Datasets description}
%
%\subsubsection{Artificial dataset}
%
%\subsubsection{UCI datasets}



%\subsection{Data preprocessing }
%To proceed with the analysis, data has to be properly processed. At first


\section{Experiments setup}


\subsection{Machine learning models}

The first machine learning algorithm that will be used in experiments is Random Forests, which are an ensemble method for classification or regression tasks by creating multiple decision trees at the training time. In this algorithm, it uses bagging and feature randomness when constructing each individual tree. And the final prediction is decided by the voting among a large number of independent trees \cite{breiman2001random}. The key concept behind this algorithm is that a large number of relatively uncorrelated trees operating as a committee will outperform any of the individual constituent models. Typically, random forests are treated as a black box model since it is nigh infeasible to gain a full understanding of the decision process by examining each tree. Commonly, the implementation of this algorithm in the scikit-learn library is adopted. 

Another black box model is Gradient Boosting Trees, which also construct an ensemble of decision trees to perform classification or regression tasks, where each decision tree is a weak prediction model. However, unlike Random Forests algorithm that fully grown decision trees are created, in Gradient Boosting Trees algorithm, each tree is a shallow tree, sometimes even as small as decision stumps (trees with two leaves). The main idea behind is to add new decision trees to the ensemble sequentially. At each iteration of the training process, those data instances with high prediction errors are emphasized by the next decision tree in order to correct the errors. And the final prediction is determined by the weighted average for each decision tree, where the weight depends on a performance of the corresponding tree \cite{natekin2013gradient}. 

There are a rich variety of libraries that implement the gradient boosting trees algorithms. In this thesis, two efficient and scalable implementations are mainly adopted, one is called XGBoost \cite{chen2016xgboost} and the other is LightGBM \cite{ke2017lightgbm}. XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable and it was a part of winning solutions of multiple machine learning competitions \cite{xgboost}. The library also works natively with scipy sparse data format and can convert it to an internal data format, called DMatrix, which speeds up the training process. Comparatively, LIghtGBM also implements fast, distributed, high performance gradient boosting algorithms. As claimed, it can outperform existing frameworks on both efficiency and accuracy with significantly lower memory consumption \cite{lightgbm}. 



deep neural network

%a function that is too complicated for any human to comprehend or (2) a function that is proprietary (Supplementary Section A). Deep learning models, for instance, tend to be black boxes of the first kind because they are highly recursive.

%Here we would just apply the GBMs to the available dataset without any manipulations on its features, or any external expert-driven knowledge involved.

\subsection{Recover patterns on artificial dataset} 

%Before exploring the local interpretation methods, we would like to justify the concept that interesting subgroups could be recovered from the artificial dataset by inspecting variable influence. Presumably, there were hidden patterns in the synthetic dataset that were useful to provide reasonable explanation to the predictions. By interpreting the effect of a certain variable, e.g. gender, it was assumed that the interesting pattern could be recovered. The procedure to construct an artificial dataset and conduct experiments will be described as follows. 
%
%For simplicity, we constructed the artificial dataset relying on the popular adult dataset, but we only extracted partial information, which meant that only the information about age, education-num, sex, hours-per-week and income were included. As assumed, the synthetic data contained some interesting patterns, such as "age < 30". One exemplary case was that when "age < 30", the attribute "gender" had stronger effect on predictions while in its complementary subgroup, the effect of "gender" was slight. And the task was indeed to discover this pattern by exploring the effect of gender. For further experiments, one way to fabricate the interesting pattern was to modify the gender effect directly on the corresponding subgroups. For instance, if the condition that "age < 30" was met, we could manually add 3 unit in terms of the scale of measurement on gender effect, and otherwise we could subtract 3 unit. Another idea was to establish two models that behaved differently when considering this condition. It is known that the coefficients in the logistic regression model have straightforward interpretation, indicating the influence level by the input features. Therefore, we could create two distinct models by changing the weights of the features in accordance with the previous defined patterns. In specific, we could assign larger weights to the model that was applied  to the pre-described subgroup to maintain larger gender effect, while decreasing feature weights on the model that was applied to its complementary subgroup.

As clarified earlier, the artificial dataset was constructed based on the Adult dataset with a hidden pattern indicating that the gender had a large impact on the prediction when "age < 30". Therefore, the aim was trying to verify whether this interesting subgroup could be recovered by pattern mining technique. 

Firstly, to measure the gender effect, we could simply use the binary flip approach described in the previous chapter. By flipping the gender value, i.e. transform from "male" to "female" or the other way around, the prediction change denoted as probability was calculated and roughly it was regarded as the effect of gender. Then, treating the effect of gender as the target concept, the subgroup discovery technique was applied to the artificial dataset to discover interesting subgroups. It could be observed that these interesting subgroups include the subgroup that was artificially generated in the dataset. The detailed results were left to the next chapter. In conclusion, it could be proved that the subgroup discovery technique could indeed provide us patterns of explanations that facilitate us to understand the predictions. 





\subsection{Comparison of different local interpretation methods}

datasets: adult, credit-g, housing
binary flip: perturbation, LIME, SHAP

numeric perturbation: perturbation, LIME, SHAP

classification vs. regression

decision tree vs. subgroup discovery

\subsection{Case Study}

real world case study 2-3 datasets