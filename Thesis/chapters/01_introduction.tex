% Assume we had a scenario where an unemployed young wanted to get a loan from the bank to start a business. The bank manager served him politely and then entered his information into the banking system, but unfortunately, the system rejected his request. Of course, the young man desired to know why his loan was rejected, and the bank was obligated to provide justifications. Hopefully, the bank manager would receive some explanations from the system telling that the bank was not willing to take risks offering a loan because of his unemployment and no assert mortgage. However, there is a high chance that the manager only obtained the final decision made by the current banking systems rather than the explanations for this decision. Thereby, how can we trust banking algorithms without explanations and how can we ensure there are no mistakes while making the decisions since the model is a complete "black box" to customers. And that is why we would like to explore variable influence to assist decision-makers to explain model decisions. 

% In this chapter, I will provide an overview of how to interpret the results generated by black box models, how this topic caught our attention, what is the purpose of this master thesis and finally how to construct the model interpretation framework. This chapter shall give readers a complete plan for this thesis. 

\subsection{Motivation}

In the recent decades, machine learning fields have been studied extensively. Simply to elucidate, machine learning is a set of methods that are used to teach computers to perform different tasks without hard-coding instructions. It has attracted much attention due to its powerful application, especially in the "Big data" era. Thanks to the boosting computational power, machine learning algorithms can make use of large volumes of data to achieve numerous tasks which are not expected before. For instance, a myriad of classification or regression tasks could be solved efficiently by applying machine learning algorithms. A simple regression task could be predicting the weather temperature by using logistic regression based on historical data, and a more complicated task could look like language translation problem. 

Since there are various kinds of machine learning models, a considerable barrier for human engineers is how to choose the right models for specific problems. Generally, concerning the evaluation of machine learning models, people tend to pay more attention on the model performance rather than the model interpretability. The model performance is definitely very fundamental to assess the model, which typically can be measured by metrics like accuracy, precision, recall and etc. Nevertheless, we should not neglect the importance of model interpretability, which shows "the degree for a human to understand model decisions and the ability to consistently predict the results"\cite{kim2016examples}. Therefore, One of the major topics to be investigated in machine learning field is interpretable machine learning. It is defined as the use of ML models for the extraction of relevant knowledge about domain relationships contained in data. \cite{murdoch2019interpretable}

Broadly speaking, machine learning models can be categorized as white box models and black box models judging from the model interpretability. White box models can be roughly considered as interpretable models, which maintain high model interpretability. Usually they contain simple structures, a limited number of model parameters, and most importantly, the decisions made by white box models are interpretable by human. For example, interpretable models include linear regression, logistic regression, and decision tree model. Those models are human-understandable since the prediction results could be interpreted by examining the model parameters. On the contrary, black box models usually have more complex structures and a substantial number of parameters which are not intrinsically understandable. Ensemble models or neural networks are normally regarded as black box models for the reason that decisions made by black box models cannot be understood by looking at their parameters, which is a major disadvantage for complex models. Typically, those complicated models can achieve better performance for the sake of less interpretability. However, proper model interpretability is crucial to provide explanations to the decisions made by the model and especially important for decision-makers. Besides, "right to explanation", meaning the right to be given an explanation for an output of automated algorithms was stated by General Data Protection Regulation(GDPR), which requires businesses to provide understandable justifications to their users \cite{voigt2017eu}. One scenario is that the bank manager is obligated to clarify reasons to the user about the loan rejection if requested. 

Since white box models are intrinsically interpretable, a more challenging problem which arises in this domain is how to explain the black box models. In other words, it is of paramount importance to investigate methods to give reasonable explanations to model predictions. Recent theoretical developments have revealed that there are approaches to interpret black box models, which can be summarized as global interpretation methods and local interpretation methods with respect to different viewpoints. As the name suggests, the global interpretation focus on the global view of the input variables. However, it would be of special interest to investigate local interpretation methods, which are operated on the instance level. Concluded by Alvarez-Melis \cite{alvarez2018robustness}, those local interpretation methods can be roughly categorized as salience-based and perturbation-based approaches. The former method category is also known as gradient-based attribution methods, computing the partial derivatives of the output with respect the each input feature, e.g. Integrated Gradients \cite{selvaraju2017grad}\cite{sundararajan2017axiomatic}. In contrast, perturbation-based approaches first generate a bunch of neighborhood data points surrounding the instance to be explained, then calculate the contribution of each input features towards the output by fitting a local interpretable model, e.g. LIME \cite{ribeiro2016model}. 

\subsection{Purpose of this thesis}

% Although the above investigation examined the ....., to the best of author knowledge, only
% few reference in the literature systematically describe the effect of. .... This was the
% motivation behind the present study.

The foremost problem we are facing is how to interpret the black box models. Undoubtedly, many interpretation methods have already come to the surface to facilitate model explanation, but they are not sufficient to deal with complex situations. The global interpretation methods give a too broad interpretation view while local interpretation methods may become too sensitive to reveal the underlying cause due to the excessive interpretation of the target instance. Indeed, the insight gained from a single instance map might be too brittle, and lead to a false sense of understanding.

Therefore, this thesis aims to develop an overarching framework to provide reasonable explanations for black box model predictions, given the urgent need to obtain decent justifications for decisions made by the algorithms. To our knowledge, many studies have yielded interpretation frameworks that are just applicable to one type of data or to a specific kind of black box model. By taking various types of data and black box models into account, it is not evident what is the best type of explanation metric and framework. We hence intend to build an interpretation framework which includes diverse explanation methods in order to align with the purpose and completeness of the targeted explanation. 

Another main contribution proposed in this thesis is that we aim to combine the local interpretation methods with the pattern mining technique since it seems not sufficient to explain model predictions by merely applying local interpretation approaches. It is convincing that black box models seem to capture the "hidden patterns" in data as to achieve good performance when performing classification or regression tasks. Similarly, we introduce the subgroup discovery technique to demonstrate the feasibility of discovering patterns that can facilitate us to explain black box models, i.e. identifying the patterns in data where a selected variable impose a significant influence. From data instance perspective, the interpretation level of this novel approach is somewhere between local view and global view, which can be considered as "pattern level" interpretation. By doing so, we can understand the behaviors of black box models by inspecting variable influence not only on the instance level interpretation but also covering the pattern level interpretation. 

In summary, the overall goal of this thesis is to develop a multifaceted interpretation framework to explain the inner behaviors of black box models, which should be furnished with various model interpretation methods. 

%Another, more general, argument commonly used against prediction-based interpretability methods is that ‘understanding’ a complex model with a single point-wise explanation is perhaps too optimistic, if not naive. Indeed, the insight gained from a single attribution or saliency map might be too brittle, and lead to a false sense of understanding. One way to address this limitation would be to go beyond points and examine the behavior of the model in a neighborhood of the point of interest.

\subsection{Thesis Structure}

The remainder of this thesis is structured as follows. 

Section 2 focuses on previous work on related fields, such as Model Interpretation methods and Subgroup Discovery field. In particular, it is dedicated to review the existing global interpretation methods and local interpretation methods. In addition, the fundamentals of subgroup discovery are discussed as well such as the selection of interestingness measure.

Section 3 is concerned with the theoretical knowledge of local interpretation methods to explain black box models. It begins with simple approaches on specific scenarios, for example, to inspect the influence of binary feature using the binary flip approach. Then the methods become more general which can be applied to any type of features, like Shapley values. Finally, more attention is laid on the novel technique which combines the approach of model agnostic local interpretation and subgroup discovery.

Section 4 presents the detailed description of datasets that are collected and the experiment set up. In experiments, the comparison of several local interpretation methods is covered. Additionally, we demonstrate case studies on specific datasets. 

Finally, Section 5 concludes the work with a summary of results and ideas on future work. 