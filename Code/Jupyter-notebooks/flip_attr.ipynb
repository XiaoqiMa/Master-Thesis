{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T09:45:07.826877Z",
     "start_time": "2019-05-08T09:45:06.952120Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T10:35:16.248114Z",
     "start_time": "2019-05-08T10:35:16.245101Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# adult_train = pd.read_csv('../Datasets/adult_train.csv', index_col=0)\n",
    "# adult_train = adult_train.replace({' ?':np.nan}).dropna()\n",
    "# adult_train = adult_train.reset_index(drop=True)\n",
    "# adult_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T10:35:18.912659Z",
     "start_time": "2019-05-08T10:35:18.906315Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# from pandas.api.types import is_string_dtype\n",
    "# from pandas.api.types import is_numeric_dtype\n",
    "# import pysnooper\n",
    "\n",
    "# # @pysnooper.snoop()\n",
    "# def flip_attr(data, attr, target, reverse_flip=False):\n",
    "#     np.random.seed(100)\n",
    "    \n",
    "#     attributes = data[attr].unique().tolist()\n",
    "#     print('chosen \"{0}\" attribute to flip, change from {1} to {2}'.format(attr, attributes[0], attributes[1]))\n",
    "#     cat_features = [i for i in data.columns if is_string_dtype(data[i])]\n",
    "#     num_features = [i for i in data.columns if is_numeric_dtype(data[i])]\n",
    "    \n",
    "#     attr_index = data.loc[data[attr] == attributes[0]].index\n",
    "#     data = data.iloc[attr_index]\n",
    "#     data[attr] = data[attr].factorize()[0]\n",
    "#     cat_data = data[cat_features]\n",
    "#     num_data = data[num_features]\n",
    "    \n",
    "#     data_flip = data.copy()\n",
    "#     data_flip[attr] = data[attr].apply(lambda x : x ^ 1)\n",
    "#     flip_num_data = data_flip[num_features]\n",
    "    \n",
    "#     X = pd.get_dummies(cat_data, drop_first=True).join(num_data, how='inner')\n",
    "#     y = target\n",
    "#     flip_X = pd.get_dummies(cat_data, drop_first=True).join(flip_num_data, how='inner')\n",
    "    \n",
    "#     rf_model = RandomForestClassifier(random_state=0).fit(X, y[attr_index])\n",
    "#     prob = np.amax(rf_model.predict_proba(X), axis=1)\n",
    "#     flip_prob = np.amax(rf_model.predict_proba(flip_X), axis=1)\n",
    "\n",
    "#     df_data = data_flip.copy()\n",
    "#     avg_effect = np.mean(flip_prob) - np.mean(prob)\n",
    "#     df_data['deviation'] = (flip_prob - prob) - avg_effect\n",
    "    \n",
    "#     return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T10:35:22.120358Z",
     "start_time": "2019-05-08T10:35:22.117746Z"
    },
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data = adult_train.drop(['native-country', 'target'], axis=1)\n",
    "# target = adult_train['target']\n",
    "# flip_attr(data, 'sex', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T10:36:11.966208Z",
     "start_time": "2019-05-08T10:36:11.912858Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pysubgroup as ps\n",
    "\n",
    "\n",
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path, index_col=0)\n",
    "    # simply drop missing values\n",
    "    data = data.dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# def train_model(data, features, target, classifier):\n",
    "#     train_data = data[[features]]\n",
    "#     target_data = data[target]\n",
    "#     model = classifier.fit(train_data.values, target_data.values.ravel())\n",
    "#     return model\n",
    "#\n",
    "# def flip_value(data, attr):\n",
    "#\n",
    "#     data[attr] = pd.get_dummies(data[attr], drop_first=True)\n",
    "#     data[attr] = data[attr].apply(lambda x: x ^ 1)\n",
    "#     return data\n",
    "\n",
    "def effect_change(data, features, attr, target, classifier):\n",
    "    train_data = pd.get_dummies(data[features], drop_first=True)\n",
    "    target_data = data[target]\n",
    "    df_attr = pd.get_dummies(data[attr], drop_first=True)\n",
    "    col_name = df_attr.columns[0]\n",
    "    t_index = df_attr.loc[df_attr[col_name] == True].index\n",
    "    f_index = df_attr.loc[df_attr[col_name] == False].index\n",
    "\n",
    "    flip_data = train_data.copy()\n",
    "    train_data = train_data.join(df_attr, how='inner')\n",
    "    flip_attr = df_attr.apply(lambda x: x ^ 1)\n",
    "\n",
    "    flip_train_data = flip_data.join(flip_attr, how='inner')\n",
    "    model = classifier.fit(train_data.values, target_data.values.ravel())\n",
    "    prob = np.amax(model.predict_proba(train_data), axis=1)\n",
    "    prob_flip = np.amax(model.predict_proba(flip_train_data), axis=1)\n",
    "\n",
    "    diff = np.mean(prob[t_index]) - np.mean(prob[f_index])\n",
    "    flip_diff = np.mean(prob_flip[f_index]) - np.mean(prob_flip[t_index])\n",
    "\n",
    "    avg_effect = flip_diff - diff\n",
    "\n",
    "    df = data[features].copy()\n",
    "    df['deviation'] = (prob_flip - prob) - avg_effect\n",
    "    # print(df.head())\n",
    "    return df\n",
    "\n",
    "\n",
    "def numeric_discovery(df):\n",
    "    target = ps.NumericTarget('deviation')\n",
    "    search_space = ps.create_nominal_selectors(df, ignore='deviation')\n",
    "    task = ps.SubgroupDiscoveryTask(df, target, search_space, qf=ps.StandardQFNumeric(1))\n",
    "    result = ps.BeamSearch().execute(task)\n",
    "\n",
    "    df_dis = ps.as_df(df, result, statistics_to_show=ps.all_statistics_numeric)\n",
    "    return df_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T10:37:58.297501Z",
     "start_time": "2019-05-08T10:37:44.943347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>size_sg</th>\n",
       "      <th>size_dataset</th>\n",
       "      <th>mean_sg</th>\n",
       "      <th>mean_dataset</th>\n",
       "      <th>std_sg</th>\n",
       "      <th>std_dataset</th>\n",
       "      <th>median_sg</th>\n",
       "      <th>median_dataset</th>\n",
       "      <th>max_sg</th>\n",
       "      <th>max_dataset</th>\n",
       "      <th>min_sg</th>\n",
       "      <th>min_dataset</th>\n",
       "      <th>mean_lift</th>\n",
       "      <th>median_lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>183.180723</td>\n",
       "      <td>marital-status= Married-civ-spouse</td>\n",
       "      <td>14065.0</td>\n",
       "      <td>30162.0</td>\n",
       "      <td>-0.172169</td>\n",
       "      <td>-0.185193</td>\n",
       "      <td>0.127375</td>\n",
       "      <td>0.11874</td>\n",
       "      <td>-0.171459</td>\n",
       "      <td>-0.187234</td>\n",
       "      <td>0.286271</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>0.929674</td>\n",
       "      <td>0.915746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172.877812</td>\n",
       "      <td>race= White AND marital-status= Married-civ-sp...</td>\n",
       "      <td>12656.0</td>\n",
       "      <td>30162.0</td>\n",
       "      <td>-0.171534</td>\n",
       "      <td>-0.185193</td>\n",
       "      <td>0.121689</td>\n",
       "      <td>0.11874</td>\n",
       "      <td>-0.171459</td>\n",
       "      <td>-0.187234</td>\n",
       "      <td>0.250481</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>-0.624949</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>0.926241</td>\n",
       "      <td>0.915746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94.716892</td>\n",
       "      <td>race= White AND education= Assoc-voc AND marit...</td>\n",
       "      <td>611.0</td>\n",
       "      <td>30162.0</td>\n",
       "      <td>-0.030174</td>\n",
       "      <td>-0.185193</td>\n",
       "      <td>0.143801</td>\n",
       "      <td>0.11874</td>\n",
       "      <td>0.033366</td>\n",
       "      <td>-0.187234</td>\n",
       "      <td>0.169269</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>-0.543737</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>0.162932</td>\n",
       "      <td>-0.178207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94.303330</td>\n",
       "      <td>education= Assoc-voc AND marital-status= Marri...</td>\n",
       "      <td>660.0</td>\n",
       "      <td>30162.0</td>\n",
       "      <td>-0.042309</td>\n",
       "      <td>-0.185193</td>\n",
       "      <td>0.155198</td>\n",
       "      <td>0.11874</td>\n",
       "      <td>0.033366</td>\n",
       "      <td>-0.187234</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>-0.555091</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>0.228461</td>\n",
       "      <td>-0.178207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.474815</td>\n",
       "      <td>race= White AND occupation= Craft-repair</td>\n",
       "      <td>3644.0</td>\n",
       "      <td>30162.0</td>\n",
       "      <td>-0.160090</td>\n",
       "      <td>-0.185193</td>\n",
       "      <td>0.097979</td>\n",
       "      <td>0.11874</td>\n",
       "      <td>-0.168100</td>\n",
       "      <td>-0.187234</td>\n",
       "      <td>0.271508</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>-0.677234</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>0.864451</td>\n",
       "      <td>0.897808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83.012614</td>\n",
       "      <td>occupation= Craft-repair</td>\n",
       "      <td>4030.0</td>\n",
       "      <td>30162.0</td>\n",
       "      <td>-0.164595</td>\n",
       "      <td>-0.185193</td>\n",
       "      <td>0.100507</td>\n",
       "      <td>0.11874</td>\n",
       "      <td>-0.187234</td>\n",
       "      <td>-0.187234</td>\n",
       "      <td>0.271508</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>-0.677234</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>0.888772</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>78.321463</td>\n",
       "      <td>race= White AND education= Assoc-voc</td>\n",
       "      <td>1142.0</td>\n",
       "      <td>30162.0</td>\n",
       "      <td>-0.116611</td>\n",
       "      <td>-0.185193</td>\n",
       "      <td>0.183168</td>\n",
       "      <td>0.11874</td>\n",
       "      <td>-0.122174</td>\n",
       "      <td>-0.187234</td>\n",
       "      <td>0.210623</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>-0.585091</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>0.629670</td>\n",
       "      <td>0.652522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77.479939</td>\n",
       "      <td>race= White AND occupation= Farming-fishing</td>\n",
       "      <td>910.0</td>\n",
       "      <td>30162.0</td>\n",
       "      <td>-0.100051</td>\n",
       "      <td>-0.185193</td>\n",
       "      <td>0.104793</td>\n",
       "      <td>0.11874</td>\n",
       "      <td>-0.118100</td>\n",
       "      <td>-0.187234</td>\n",
       "      <td>0.173719</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>-0.548186</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>0.540249</td>\n",
       "      <td>0.630760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>75.780733</td>\n",
       "      <td>occupation= Farming-fishing</td>\n",
       "      <td>989.0</td>\n",
       "      <td>30162.0</td>\n",
       "      <td>-0.108570</td>\n",
       "      <td>-0.185193</td>\n",
       "      <td>0.105681</td>\n",
       "      <td>0.11874</td>\n",
       "      <td>-0.125962</td>\n",
       "      <td>-0.187234</td>\n",
       "      <td>0.173719</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>-0.548186</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>0.586251</td>\n",
       "      <td>0.672750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75.475822</td>\n",
       "      <td>occupation= Prof-specialty AND marital-status=...</td>\n",
       "      <td>2066.0</td>\n",
       "      <td>30162.0</td>\n",
       "      <td>-0.148661</td>\n",
       "      <td>-0.185193</td>\n",
       "      <td>0.088634</td>\n",
       "      <td>0.11874</td>\n",
       "      <td>-0.138323</td>\n",
       "      <td>-0.187234</td>\n",
       "      <td>0.210623</td>\n",
       "      <td>0.312766</td>\n",
       "      <td>-0.585091</td>\n",
       "      <td>-0.687234</td>\n",
       "      <td>0.802734</td>\n",
       "      <td>0.738774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      quality                                           subgroup  size_sg  \\\n",
       "0  183.180723                 marital-status= Married-civ-spouse  14065.0   \n",
       "1  172.877812  race= White AND marital-status= Married-civ-sp...  12656.0   \n",
       "2   94.716892  race= White AND education= Assoc-voc AND marit...    611.0   \n",
       "3   94.303330  education= Assoc-voc AND marital-status= Marri...    660.0   \n",
       "4   91.474815           race= White AND occupation= Craft-repair   3644.0   \n",
       "5   83.012614                           occupation= Craft-repair   4030.0   \n",
       "6   78.321463               race= White AND education= Assoc-voc   1142.0   \n",
       "7   77.479939        race= White AND occupation= Farming-fishing    910.0   \n",
       "8   75.780733                        occupation= Farming-fishing    989.0   \n",
       "9   75.475822  occupation= Prof-specialty AND marital-status=...   2066.0   \n",
       "\n",
       "   size_dataset   mean_sg  mean_dataset    std_sg  std_dataset  median_sg  \\\n",
       "0       30162.0 -0.172169     -0.185193  0.127375      0.11874  -0.171459   \n",
       "1       30162.0 -0.171534     -0.185193  0.121689      0.11874  -0.171459   \n",
       "2       30162.0 -0.030174     -0.185193  0.143801      0.11874   0.033366   \n",
       "3       30162.0 -0.042309     -0.185193  0.155198      0.11874   0.033366   \n",
       "4       30162.0 -0.160090     -0.185193  0.097979      0.11874  -0.168100   \n",
       "5       30162.0 -0.164595     -0.185193  0.100507      0.11874  -0.187234   \n",
       "6       30162.0 -0.116611     -0.185193  0.183168      0.11874  -0.122174   \n",
       "7       30162.0 -0.100051     -0.185193  0.104793      0.11874  -0.118100   \n",
       "8       30162.0 -0.108570     -0.185193  0.105681      0.11874  -0.125962   \n",
       "9       30162.0 -0.148661     -0.185193  0.088634      0.11874  -0.138323   \n",
       "\n",
       "   median_dataset    max_sg  max_dataset    min_sg  min_dataset  mean_lift  \\\n",
       "0       -0.187234  0.286271     0.312766 -0.687234    -0.687234   0.929674   \n",
       "1       -0.187234  0.250481     0.312766 -0.624949    -0.687234   0.926241   \n",
       "2       -0.187234  0.169269     0.312766 -0.543737    -0.687234   0.162932   \n",
       "3       -0.187234  0.171100     0.312766 -0.555091    -0.687234   0.228461   \n",
       "4       -0.187234  0.271508     0.312766 -0.677234    -0.687234   0.864451   \n",
       "5       -0.187234  0.271508     0.312766 -0.677234    -0.687234   0.888772   \n",
       "6       -0.187234  0.210623     0.312766 -0.585091    -0.687234   0.629670   \n",
       "7       -0.187234  0.173719     0.312766 -0.548186    -0.687234   0.540249   \n",
       "8       -0.187234  0.173719     0.312766 -0.548186    -0.687234   0.586251   \n",
       "9       -0.187234  0.210623     0.312766 -0.585091    -0.687234   0.802734   \n",
       "\n",
       "   median_lift  \n",
       "0     0.915746  \n",
       "1     0.915746  \n",
       "2    -0.178207  \n",
       "3    -0.178207  \n",
       "4     0.897808  \n",
       "5     1.000000  \n",
       "6     0.652522  \n",
       "7     0.630760  \n",
       "8     0.672750  \n",
       "9     0.738774  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_train = pd.read_csv('../Datasets/adult_train.csv', index_col=0)\n",
    "adult_train = adult_train.replace({' ?':np.nan}).dropna()\n",
    "adult_train = adult_train.reset_index(drop=True)\n",
    "\n",
    "features = ['education', 'occupation', 'race', 'marital-status']\n",
    "target = 'target'\n",
    "attr = 'sex'\n",
    "classifier = RandomForestClassifier(random_state=0)\n",
    "df = effect_change(adult_train, features, attr, target, classifier)\n",
    "numeric_discovery(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
